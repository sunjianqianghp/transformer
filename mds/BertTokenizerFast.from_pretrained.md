## 1. ä½œç”¨

`BertTokenizerFast.from_pretrained` ç”¨æ¥**åŠ è½½ä¸€ä¸ªå·²ç»è®­ç»ƒå¥½çš„ BERT åˆ†è¯å™¨ï¼ˆTokenizerï¼‰**ï¼Œé€šå¸¸æ˜¯ä» Hugging Face Model Hub æˆ–æœ¬åœ°æ–‡ä»¶åŠ è½½ã€‚
å®ƒçš„ä¸»è¦åŠŸèƒ½æ˜¯ï¼š

* æŠŠè‡ªç„¶è¯­è¨€æ–‡æœ¬è½¬æˆ **token IDs**ï¼ˆæ•´æ•°åºåˆ—ï¼‰ä¾›æ¨¡å‹è¾“å…¥
* æŠŠæ¨¡å‹è¾“å‡ºçš„ token IDs è½¬å›åŸå§‹æ–‡æœ¬
* è‡ªåŠ¨åŠ è½½ **è¯è¡¨ï¼ˆvocabï¼‰**ã€**åˆ†è¯è§„åˆ™**ã€**ç‰¹æ®Šç¬¦å·** ç­‰ä¿¡æ¯
* `Fast` ç‰ˆæœ¬åŸºäº **ğŸ¤— Tokenizers** åº“ï¼ˆRust å®ç°ï¼‰ï¼Œæ¯”æ—§ç‰ˆ Python åˆ†è¯å™¨ **å¿«å¾ˆå¤š**ï¼Œæ”¯æŒæ›´å¤šå¹¶è¡ŒåŒ–

---

## 2. åŸºæœ¬ç”¨æ³•

### ä» Hugging Face Hub åŠ è½½

```python
from transformers import BertTokenizerFast

tokenizer = BertTokenizerFast.from_pretrained("bert-base-cased")
```

è¿™æ ·ä¼šï¼š

1. ä» Hugging Face Hub ä¸‹è½½ `bert-base-cased` çš„åˆ†è¯å™¨é…ç½®æ–‡ä»¶ï¼ˆ`tokenizer.json`ã€`vocab.txt` ç­‰ï¼‰åˆ°ç¼“å­˜
2. è¿”å›ä¸€ä¸ªå¯ç›´æ¥ä½¿ç”¨çš„ `BertTokenizerFast` å¯¹è±¡

---

### ä»æœ¬åœ°ç›®å½•åŠ è½½

```python
tokenizer = BertTokenizerFast.from_pretrained("./bert-base-cased-local")
```

æœ¬åœ°ç›®å½•éœ€è¦åŒ…å«ï¼š

* `vocab.txt`ï¼ˆè¯è¡¨ï¼‰
* `tokenizer_config.json`ï¼ˆåˆ†è¯å™¨é…ç½®ï¼‰
* `special_tokens_map.json`ï¼ˆç‰¹æ®Š token æ˜ å°„ï¼‰
* `tokenizer.json`ï¼ˆFast åˆ†è¯å™¨å®šä¹‰ï¼‰

---

## 3. å¸¸ç”¨å‚æ•°

`BertTokenizerFast.from_pretrained` çš„å¸¸ç”¨å‚æ•°ï¼ˆç»§æ‰¿è‡ª `PreTrainedTokenizerFast.from_pretrained`ï¼‰ï¼š

| å‚æ•°                              | ä½œç”¨                        |
| ------------------------------- | ------------------------- |
| `pretrained_model_name_or_path` | æ¨¡å‹åç§°ï¼ˆHubï¼‰æˆ–æœ¬åœ°è·¯å¾„            |
| `cache_dir`                     | æŒ‡å®šç¼“å­˜ç›®å½•                    |
| `use_fast`                      | æ˜¯å¦åŠ è½½ Fast ç‰ˆæœ¬ï¼ˆTrue/Falseï¼‰  |
| `revision`                      | æŒ‡å®šæ¨¡å‹ç‰ˆæœ¬ï¼ˆbranchã€tagã€commitï¼‰ |
| `do_lower_case`                 | æ˜¯å¦å…¨éƒ¨è½¬å°å†™ï¼ˆcased æ¨¡å‹è¦è®¾ Falseï¼‰ |
| `local_files_only`              | True æ—¶åªç”¨æœ¬åœ°æ–‡ä»¶ï¼Œä¸è”ç½‘          |
| `force_download`                | å¼ºåˆ¶é‡æ–°ä¸‹è½½                    |

---

## 4. ä½¿ç”¨ç¤ºä¾‹

```python
from transformers import BertTokenizerFast

# 1. åŠ è½½åˆ†è¯å™¨
tokenizer = BertTokenizerFast.from_pretrained("bert-base-cased")

# 2. ç¼–ç æ–‡æœ¬
text = "Hello World!"
encoded = tokenizer(text, return_tensors="pt")  
print(encoded)
# è¾“å‡ºï¼š{'input_ids': tensor([[101, 8667, 1362, 106, 102]]), 'token_type_ids': tensor([[0,0,0,0,0]]), 'attention_mask': tensor([[1,1,1,1,1]])}
# token_type_idså°±æ˜¯segment_id, attention_mask:1è¡¨ç¤ºæœ‰æ•ˆtokenï¼Œ0è¡¨ç¤ºæ— æ•ˆçš„padding token

# 3. è§£ç  ID
decoded = tokenizer.decode(encoded["input_ids"][0])
print(decoded)  # "Hello World!"
```

---

## 5. ä½œç”¨æ€»ç»“

* **åŠ è½½åˆ†è¯è§„åˆ™**ï¼šä¸å¿…æ‰‹åŠ¨é…ç½®è¯è¡¨å’Œåˆ‡åˆ†æ–¹å¼
* **å¯ç›´æ¥ç”¨äºæ¨¡å‹**ï¼šä¸ `AutoModel` ç­‰æ¨¡å‹ç±»æ— ç¼è¡”æ¥
* **é«˜æ€§èƒ½**ï¼š`Fast` ç‰ˆæœ¬åŸºäº Rust å®ç°ï¼Œæ¯”æ™®é€š Python åˆ†è¯å™¨å¿«å‡ åå€ï¼Œå°¤å…¶åœ¨æ‰¹é‡å¤„ç†æ—¶

---

å¦‚æœä½ æ„¿æ„ï¼Œæˆ‘å¯ä»¥å¸®ä½ ç”»ä¸€ä¸ª **BERT åˆ†è¯æµç¨‹å›¾**ï¼Œä» `text â†’ tokens â†’ IDs â†’ æ¨¡å‹` å†åˆ° `IDs â†’ text`ï¼Œè®© `BertTokenizerFast.from_pretrained` çš„ä½œç”¨ä¸€çœ¼å°±èƒ½çœ‹æ‡‚ã€‚ä½ è¦æˆ‘ç”»å—ï¼Ÿ
