`evaluate.load()` æ˜¯ ğŸ¤— Hugging Face **Evaluate** åº“ä¸­çš„ä¸€ä¸ªæ ¸å¿ƒå‡½æ•°ï¼Œä¸»è¦ä½œç”¨æ˜¯**åŠ è½½è¯„ä¼°æŒ‡æ ‡ï¼ˆmetricsï¼‰æˆ–è¯„ä¼°æ¨¡å—ï¼ˆevaluation modulesï¼‰**ï¼Œè®©ä½ åœ¨ NLPã€CV ç­‰ä»»åŠ¡ä¸­æ–¹ä¾¿åœ°è®¡ç®—å¸¸è§æŒ‡æ ‡ï¼Œæ¯”å¦‚ `accuracy`ã€`f1`ã€`rouge`ã€`bleu` ç­‰ã€‚

---

## 1ï¸âƒ£ ä½œç”¨

* ä» **Hugging Face Hub** æˆ–æœ¬åœ°åŠ è½½è¯„ä¼°æŒ‡æ ‡
* ç»Ÿä¸€ä¸åŒæŒ‡æ ‡çš„ä½¿ç”¨æ–¹å¼ï¼ˆ`.compute()`ï¼‰
* æ–¹ä¾¿åœ¨æ¨¡å‹è®­ç»ƒå’Œè¯„ä¼°ä¸­è®¡ç®—å‡†ç¡®ç‡ã€F1ã€ROUGE ç­‰æŒ‡æ ‡
* æ”¯æŒè‡ªå®šä¹‰æŒ‡æ ‡ï¼ˆä»æœ¬åœ° Python è„šæœ¬åŠ è½½ï¼‰

---

## 2ï¸âƒ£ åŸºæœ¬ç”¨æ³•

```python
import evaluate

# åŠ è½½ä¸€ä¸ªå†…ç½®æŒ‡æ ‡ï¼Œæ¯”å¦‚å‡†ç¡®ç‡
accuracy = evaluate.load("accuracy")

# æ¨¡æ‹Ÿé¢„æµ‹å’Œæ ‡ç­¾
y_pred = [0, 1, 1, 0]
y_true = [0, 1, 0, 0]

# è®¡ç®—æŒ‡æ ‡
results = accuracy.compute(predictions=y_pred, references=y_true)
print(results)
```

è¾“å‡ºï¼š

```python
{'accuracy': 0.75}
```

---

## 3ï¸âƒ£ å‚æ•°è¯´æ˜

```python
evaluate.load(path, config_name=None, module_type=None, cache_dir=None, download_mode=None)
```

| å‚æ•°                 | è¯´æ˜                                                                 |
| ------------------ | ------------------------------------------------------------------ |
| **path**           | è¯„ä¼°æ¨¡å—åç§°ï¼ˆå¦‚ `"accuracy"`ã€`"f1"`ã€`"bleu"`ï¼‰æˆ–æœ¬åœ°è·¯å¾„                        |
| **config\_name**   | æœ‰äº›æŒ‡æ ‡æœ‰å¤šç§é…ç½®ï¼Œæ¯”å¦‚ `"glue"` æ•°æ®é›†çš„ä¸åŒä»»åŠ¡ï¼š`evaluate.load("glue", "rte")`      |
| **module\_type**   | `"metric"` æˆ– `"comparison"`ï¼Œä¸€èˆ¬ä¸ç”¨æ‰‹åŠ¨è®¾                                |
| **cache\_dir**     | ç¼“å­˜ç›®å½•ï¼ˆä¸æŒ‡å®šåˆ™é»˜è®¤åˆ° `~/.cache/huggingface/modules/evaluate_metrics/`ï¼‰     |
| **download\_mode** | æ˜¯å¦å¼ºåˆ¶é‡æ–°ä¸‹è½½ï¼Œé€‰é¡¹ï¼š`"reuse_dataset_if_exists"`ï¼ˆé»˜è®¤ï¼‰ã€`"force_redownload"` ç­‰ |

---

## 4ï¸âƒ£ å…¸å‹ç”¨ä¾‹

### (1) åŠ è½½å¤šä¸ªæŒ‡æ ‡

```python
f1_metric = evaluate.load("f1")
precision_metric = evaluate.load("precision")

preds = [0, 1, 1, 0]
labels = [0, 1, 0, 0]

print(f1_metric.compute(predictions=preds, references=labels, average="macro"))
print(precision_metric.compute(predictions=preds, references=labels, average="macro"))
```

---

### (2) è¯„ä¼° GLUE ä¸­çš„æŸä¸ªä»»åŠ¡

```python
glue_metric = evaluate.load("glue", "rte")
preds = [0, 1, 1, 0]
labels = [0, 1, 0, 0]
print(glue_metric.compute(predictions=preds, references=labels))
```

---

### (3) ä»æœ¬åœ° Python è„šæœ¬åŠ è½½è‡ªå®šä¹‰æŒ‡æ ‡

```python
custom_metric = evaluate.load("./my_metrics/my_accuracy.py")
result = custom_metric.compute(predictions=[1,0], references=[1,1])
print(result)
```

å…¶ä¸­ `my_accuracy.py` å¿…é¡»å®šä¹‰ `compute()` å‡½æ•°è¿”å›ç»“æœå­—å…¸ã€‚

---

## 5ï¸âƒ£ æ³¨æ„äº‹é¡¹

* `evaluate` æ˜¯ç‹¬ç«‹äº `datasets` çš„åº“ï¼ˆä½†è¯­æ³•é£æ ¼ç›¸ä¼¼ï¼‰
* å¦‚æœæ— æ³•ä¸‹è½½æŒ‡æ ‡ï¼ˆç½‘ç»œé—®é¢˜ï¼‰ï¼Œå¯ä»¥æ‰‹åŠ¨ä¸‹è½½åˆ°æœ¬åœ°ï¼Œç„¶åç”¨ `evaluate.load(local_path)` åŠ è½½
* ä¸€äº›ä»»åŠ¡å‹æŒ‡æ ‡ï¼ˆå¦‚ `glue`ã€`squad`ï¼‰éœ€è¦ä¼  `config_name`

---

å¦‚æœä½ éœ€è¦çš„è¯ï¼Œæˆ‘å¯ä»¥å¸®ä½ æ•´ç†ä¸€ä»½ **evaluate å¸¸ç”¨æŒ‡æ ‡é€ŸæŸ¥è¡¨**ï¼Œè¿™æ ·ä½ ä¸ç”¨æ¯æ¬¡å»æŸ¥ Hugging Face æ–‡æ¡£å°±èƒ½ç›´æ¥è°ƒç”¨ã€‚
ä½ éœ€è¦æˆ‘å¸®ä½ æ•´ç†å—ï¼Ÿè¿™æ ·ä¼šæ›´æ–¹ä¾¿ä½ ç›´æ¥ç”¨ã€‚
