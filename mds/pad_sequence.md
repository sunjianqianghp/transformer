`torch.nn.utils.rnn.pad_sequence` æ˜¯ PyTorch ä¸­ä¸€ä¸ªéå¸¸å®ç”¨çš„å·¥å…·å‡½æ•°ï¼Œä¸»è¦ç”¨äºå°†**ä¸åŒé•¿åº¦çš„åºåˆ—**è¡¥é½ï¼ˆpaddingï¼‰ä¸ºç»Ÿä¸€é•¿åº¦ï¼Œæ–¹ä¾¿æ‰¹å¤„ç†ã€‚

è¿™ä¸ªå‡½æ•°åœ¨å¤„ç† **å˜é•¿åºåˆ—ï¼ˆvariable-length sequencesï¼‰** æ—¶éå¸¸å¸¸ç”¨ï¼Œå°¤å…¶æ˜¯åœ¨ NLPã€åºåˆ—å»ºæ¨¡ã€RNN è¾“å…¥ç­‰åœºæ™¯ä¸­ã€‚

---

## ğŸ§© å‡½æ•°ä½œç”¨

**å°†å¤šä¸ªä¸åŒé•¿åº¦çš„åºåˆ—è¡¥é½ä¸ºç›¸åŒé•¿åº¦çš„å¼ é‡ï¼ˆTensorï¼‰ï¼Œä»¥ä¾¿å¯ä»¥ç»„æˆä¸€ä¸ª batchã€‚**

---

## ğŸ”§ è¯­æ³•

```python
torch.nn.utils.rnn.pad_sequence(sequences, batch_first=False, padding_value=0.0)
```

---

## âœ… å‚æ•°è¯´æ˜

| å‚æ•°              | è¯´æ˜                                                        |
| --------------- | --------------------------------------------------------- |
| `sequences`     | ä¸€ä¸ªåŒ…å«å¤šä¸ª 1D æˆ– 2D å¼ é‡çš„ list æˆ– tupleï¼Œæ¯ä¸ªå¼ é‡ä»£è¡¨ä¸€ä¸ªåºåˆ—ï¼Œé•¿åº¦å¯å˜           |
| `batch_first`   | æ˜¯å¦å°† batch ç»´æ”¾åœ¨ç¬¬ä¸€ä¸ªç»´åº¦ï¼ˆ`True` ä¸º `[batch_size, seq_len, ...]`ï¼‰ |
| `padding_value` | ç”¨äºå¡«å……çš„å€¼ï¼ˆé»˜è®¤æ˜¯ 0.0ï¼‰                                           |

---

## ğŸ“Œ ç¤ºä¾‹

### ğŸ”¤ ä¸¾ä¾‹ï¼šå¤„ç†å˜é•¿å¥å­

```python
import torch
from torch.nn.utils.rnn import pad_sequence

# å‡è®¾æœ‰ä¸‰ä¸ªä¸åŒé•¿åº¦çš„åºåˆ—ï¼ˆæ¯ä¸ªæ˜¯ä¸€ä¸ª Tensorï¼‰
seq1 = torch.tensor([1, 2, 3])
seq2 = torch.tensor([4, 5])
seq3 = torch.tensor([6])

# æ”¾åœ¨ list ä¸­
sequences = [seq1, seq2, seq3]

# è¿›è¡Œ paddingï¼ˆé»˜è®¤æ˜¯ seq_len åœ¨ç¬¬ä¸€ç»´ï¼‰
padded = pad_sequence(sequences, batch_first=True, padding_value=0)

print(padded)
```

### ğŸ”¢ è¾“å‡ºç»“æœï¼š

```python
tensor([[1, 2, 3],
        [4, 5, 0],
        [6, 0, 0]])
```

* æ¯è¡Œæ˜¯ä¸€ä¸ªåºåˆ—ï¼Œå·²ç»è¡¥é½åˆ°ç­‰é•¿ã€‚
* å¡«å……ä½ç½®ä½¿ç”¨çš„æ˜¯ `0`ï¼ˆé»˜è®¤æˆ–è‡ªå®šä¹‰çš„ padding\_valueï¼‰ã€‚

---

## ğŸ“ ç»´åº¦è¯´æ˜

| å‚æ•°è®¾ç½®                | è¾“å‡ºç»´åº¦                        |
| ------------------- | --------------------------- |
| `batch_first=True`  | `[batch_size, max_seq_len]` |
| `batch_first=False` | `[max_seq_len, batch_size]` |

---

## ğŸš€ é€šå¸¸é…åˆä½¿ç”¨åœºæ™¯

### é…åˆä»¥ä¸‹æ¨¡å—ä½¿ç”¨ï¼š

* `torch.nn.Embedding`ï¼ˆè¿›è¡Œè¯å‘é‡æ˜ å°„ï¼‰
* `torch.nn.utils.rnn.pack_padded_sequence`ï¼ˆRNN ä¸­è¿›ä¸€æ­¥å‹ç¼© paddingï¼‰
* `torch.nn.LSTM` / `GRU`ï¼ˆåœ¨å¤„ç†å˜é•¿åºåˆ—æ—¶ï¼‰

---

## ğŸ”„ ç¤ºä¾‹ï¼šé…åˆ LSTM ä½¿ç”¨

```python
from torch.nn.utils.rnn import pad_sequence, pack_padded_sequence

# å‡è®¾æœ‰åµŒå…¥åçš„å˜é•¿åºåˆ—
embedded = [torch.randn(l, 10) for l in [5, 3, 2]]  # 3ä¸ªå¥å­ï¼Œé•¿åº¦åˆ†åˆ«ä¸º5, 3, 2

# padding
padded = pad_sequence(embedded, batch_first=True)  # shape: [3, 5, 10]

# åºåˆ—åŸå§‹é•¿åº¦ï¼ˆå¿…è¦ï¼‰
lengths = torch.tensor([5, 3, 2])

# pack æˆ PackedSequence å¯¹è±¡ï¼ˆå–‚ç»™ LSTMï¼‰
packed = pack_padded_sequence(padded, lengths, batch_first=True, enforce_sorted=False)
```

---

## âœ… æ€»ç»“

| ä¼˜ç‚¹        | è¯´æ˜                                           |
| --------- | -------------------------------------------- |
| ğŸ§© å¯¹é½åºåˆ—   | æŠŠä¸åŒé•¿åº¦çš„åºåˆ—è¡¥é½æˆç»Ÿä¸€é•¿åº¦ï¼Œæ–¹ä¾¿æ‰¹é‡å¤„ç†                       |
| ğŸ”— å…¼å®¹RNN  | å¸¸ä¸ `pack_padded_sequence` è¿ç”¨ï¼Œä»¥æ”¯æŒå˜é•¿åºåˆ—         |
| âš™ï¸ è‡ªå®šä¹‰å¡«å……å€¼ | å¯è‡ªå®šä¹‰ `padding_value`ï¼Œé€‚é…ä¸åŒåœºæ™¯ï¼ˆå¦‚ `<PAD>` tokenï¼‰ |
| ğŸ“ çµæ´»ç»´åº¦   | æ”¯æŒ `batch_first` æ§åˆ¶ç»´åº¦é¡ºåº                      |

---

å¦‚æœä½ æƒ³æˆ‘å†æ¼”ç¤ºä¸€æ®µå®Œæ•´çš„ NLP ç¤ºä¾‹ï¼ˆä¾‹å¦‚å¤„ç†ä¸€æ‰¹å¥å­æ–‡æœ¬ï¼‰ï¼Œä¹Ÿå¯ä»¥å‘Šè¯‰æˆ‘ï¼Œæˆ‘å¯ä»¥ä» `tokenize` åˆ° `Embedding + LSTM` ä¸€æ­¥æ­¥å¸®ä½ å±•ç¤ºã€‚æ˜¯å¦éœ€è¦ï¼Ÿ
