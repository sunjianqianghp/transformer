`datasets.load_dataset` æ˜¯ Hugging Face **ğŸ¤— Datasets** åº“çš„æ ¸å¿ƒæ–¹æ³•ä¹‹ä¸€ï¼Œç”¨æ¥**åŠ è½½æ•°æ®é›†**ï¼ˆåŒ…æ‹¬å®˜æ–¹æ•°æ®é›†ã€æœ¬åœ°æ–‡ä»¶ã€ç§æœ‰æ•°æ®é›†ã€ç”šè‡³è‡ªå®šä¹‰æ•°æ®ï¼‰ï¼Œå¹¶è¿”å›ä¸€ä¸ª `Dataset` æˆ– `DatasetDict` å¯¹è±¡ï¼Œæ–¹ä¾¿åœ¨ NLPã€CVã€éŸ³é¢‘ç­‰ä»»åŠ¡ä¸­ç›´æ¥ä½¿ç”¨ã€‚

---

## 1ï¸âƒ£ åŸºæœ¬ä½œç”¨

* **å¿«é€Ÿè·å–** Hugging Face Hub ä¸Šçš„å¼€æºæ•°æ®é›†ï¼ˆå¦‚ `imdb`, `squad`, `glue` ç­‰ï¼‰
* ä» **æœ¬åœ°æ–‡ä»¶**ï¼ˆCSVã€JSONã€Parquetã€æ–‡æœ¬ç­‰ï¼‰è¯»å–æ•°æ®
* æ”¯æŒ **è¿œç¨‹æ–‡ä»¶ URL**
* æ”¯æŒ **æ•°æ®åˆ‡åˆ†ï¼ˆsplitï¼‰**ã€æµå¼è¯»å–ï¼ˆstreamingï¼‰
* è¿”å›çš„æ•°æ®å¯ä»¥ç›´æ¥ç”¨ **PyTorch**ã€**TensorFlow**ã€**NumPy** ç­‰æ¡†æ¶å¤„ç†

---

## 2ï¸âƒ£ åŸºæœ¬ç”¨æ³•

### ğŸ’¡ ä» Hugging Face Hub åŠ è½½

```python
from datasets import load_dataset

# åŠ è½½ IMDb å½±è¯„æ•°æ®é›†
dataset = load_dataset("imdb")

print(dataset)  
# DatasetDict({
#     train: Dataset({
#         features: ['text', 'label'],
#         num_rows: 25000
#     })
#     test: Dataset(...)
#     unsupervised: Dataset(...)
# })
```

---

### ğŸ’¡ åŠ è½½æŒ‡å®šçš„ split

```python
train_data = load_dataset("imdb", split="train")
print(train_data[0])  # æ‰“å°ç¬¬ä¸€æ¡æ ·æœ¬
```

---

### ğŸ’¡ ä»æœ¬åœ°æ–‡ä»¶åŠ è½½

```python
dataset = load_dataset("csv", data_files="data/mydata.csv")
```

æ”¯æŒå¤šæ–‡ä»¶ï¼š

```python
dataset = load_dataset("csv", data_files={"train": "train.csv", "test": "test.csv"})
```

---

### ğŸ’¡ ä» URL åŠ è½½

```python
dataset = load_dataset("csv", data_files="https://example.com/data.csv")
```

---

### ğŸ’¡ æŒ‡å®šæ•°æ®å­é›†ï¼ˆsubsetï¼‰

æœ‰äº›æ•°æ®é›†æœ‰å¤šä¸ªå­é›†ï¼ˆå¦‚ `glue`ï¼‰ï¼š

```python
dataset = load_dataset("glue", "mrpc")
```

---

### ğŸ’¡ æµå¼åŠ è½½ï¼ˆå¤§æ•°æ®é›†ï¼‰

```python
dataset = load_dataset("imdb", split="train", streaming=True)
for example in dataset:
    print(example)
    break
```

---

## 3ï¸âƒ£ è¿”å›å¯¹è±¡è¯´æ˜

`load_dataset` è¿”å›ï¼š

* **DatasetDict**ï¼ˆå­—å…¸å½¢å¼ï¼ŒåŒ…å«å¤šä¸ª splitï¼Œå¦‚ train/test/validationï¼‰
* **Dataset**ï¼ˆå•ä¸ªæ•°æ®é›† splitï¼‰

ç‰¹ç‚¹ï¼š

* æ”¯æŒ **ç´¢å¼•è®¿é—®**ï¼ˆ`dataset[0]`ï¼‰
* æ”¯æŒ **map/filter/shuffle** ç­‰é«˜æ•ˆå¤„ç†
* æ”¯æŒé›¶æ‹·è´ï¼ˆArrow æ ¼å¼ï¼‰ï¼Œé€Ÿåº¦å¿«ã€å†…å­˜å ç”¨ä½

---

## 4ï¸âƒ£ å¸¸ç”¨å‚æ•°

| å‚æ•°           | ä½œç”¨                  |
| ------------ | ------------------- |
| `path`       | æ•°æ®é›†åç§°ã€æœ¬åœ°è·¯å¾„ã€URLã€è„šæœ¬è·¯å¾„ |
| `name`       | å­é›†åç§°ï¼ˆsubsetï¼‰        |
| `split`      | åŠ è½½ç‰¹å®šæ•°æ®åˆ‡åˆ†            |
| `data_files` | æœ¬åœ°æˆ– URL æ–‡ä»¶è·¯å¾„        |
| `cache_dir`  | æ•°æ®ç¼“å­˜è·¯å¾„              |
| `streaming`  | æ˜¯å¦æµå¼åŠ è½½              |
| `revision`   | ç‰ˆæœ¬ï¼ˆGit æ ‡ç­¾/åˆ†æ”¯ï¼‰       |
| `token`      | è®¿é—®ç§æœ‰æ•°æ®é›†çš„ Token      |

---

## 5ï¸âƒ£ ç¤ºä¾‹ï¼šæ–‡æœ¬åˆ†ç±»ä»»åŠ¡

```python
from datasets import load_dataset

dataset = load_dataset("ag_news")
train_data = dataset["train"]
test_data = dataset["test"]

print(train_data.features)  # æŸ¥çœ‹å­—æ®µä¿¡æ¯
print(train_data[0])        # æŸ¥çœ‹ç¬¬ä¸€æ¡æ•°æ®
```

---

å¦‚æœä½ éœ€è¦ï¼Œæˆ‘å¯ä»¥å¸®ä½ ç”»ä¸€ä¸ª **`load_dataset` æ•°æ®åŠ è½½æµç¨‹å›¾**ï¼Œå±•ç¤ºä» Hugging Face Hubã€æœ¬åœ°æ–‡ä»¶ã€URL åˆ°æœ€ç»ˆ `DatasetDict` çš„æ•´ä¸ªè¿‡ç¨‹ï¼Œä¼šæ›´ç›´è§‚ã€‚ä½ è¦æˆ‘ç”»å—ï¼Ÿ
